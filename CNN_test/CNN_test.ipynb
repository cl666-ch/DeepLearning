{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "# 深度学习模型实现与测试\n",
    "\n",
    "本笔记本包含多个卷积神经网络(CNN)深度学习模型的实现，包括 LeNet、AlexNet、VGG、NiN、GoogLeNet、ResNet 和 DenseNet。每个模型都可以单独测试，并使用 FashionMNIST 数据集进行训练和评估。\n",
    "\n",
    "以下是主要内容：\n",
    "- 使用 PyTorch 实现经典的深度学习模型。\n",
    "- 提供模型训练和评估的通用函数。\n",
    "- 测试不同模型的性能。\n",
    "\n",
    "请根据需要选择模型并运行相应的代码块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过Sequential类来实现LeNet模型，卷积神经网络LeNet（2个卷积层，3个全连接层）(使用批量归一化层)\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Sequential(\n",
    "            nn.Conv2d(1,6,5),\n",
    "            #nn.BatchNorm2d(6),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(6,16,5),\n",
    "            #nn.BatchNorm2d(16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(16*4*4,120),\n",
    "            #nn.BatchNorm1d(120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120,84),\n",
    "            #nn.BatchNorm1d(84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84,10)\n",
    "        )\n",
    "    def forward(self,img):\n",
    "        feature = self.conv(img)\n",
    "        output=self.fc(feature.view(img.shape[0],-1))\n",
    "        return output\n",
    "\n",
    "#深度神经网络AlexNet(5个卷积层，3个全连接层)\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Sequential(\n",
    "            nn.Conv2d(1,96,11,4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3,2),\n",
    "            nn.Conv2d(96,256,5,1,2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3,2),\n",
    "            nn.Conv2d(256,384,3,1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384,384,3,1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384,256,3,1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3,2)\n",
    "        )\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(256*5*5,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096,10)\n",
    "        )\n",
    "    def forward(self,img):\n",
    "        feature=self.conv(img)\n",
    "        output=self.fc(feature.view(img.shape[0],-1))\n",
    "        return output\n",
    "\n",
    "#使用重复元素的网络VGG-11(8个卷积层，三个全连接层)\n",
    "class VGG(nn.Module):\n",
    "    ratio=8\n",
    "    conv_arch=((1,1,64//ratio),(1,64//ratio,128//ratio),(2,128//ratio,256//ratio),(2,256//ratio,512//ratio),(2,512//ratio,512//ratio))\n",
    "    fc_features=512*7*7//ratio\n",
    "    fc_hidden_units=4096//ratio\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv=nn.Sequential()\n",
    "        for i,(num_convs,in_channels,out_channels) in enumerate(self.conv_arch):\n",
    "            self.conv.add_module(\"vgg_block_\"+str(i+1),self.vgg_block(num_convs,in_channels,out_channels))\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(self.fc_features,self.fc_hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.fc_hidden_units,self.fc_hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.fc_hidden_units,10)\n",
    "        )\n",
    "    def forward(self,img):\n",
    "        features=self.conv(img)\n",
    "        output=self.fc(features.view(img.shape[0],-1))\n",
    "        return output\n",
    "\n",
    "    def vgg_block(self,num_convs, in_channels,out_channels):\n",
    "        blk=[]\n",
    "        for i in range(num_convs):\n",
    "            if i==0:\n",
    "                blk.append(nn.Conv2d(in_channels,out_channels,3,padding=1))\n",
    "            else:\n",
    "                blk.append(nn.Conv2d(out_channels,out_channels,3,padding=1))\n",
    "            blk.append(nn.ReLU())\n",
    "        blk.append(nn.MaxPool2d(2,2))\n",
    "        return nn.Sequential(*blk)\n",
    "\n",
    "#网络中的网络(NiN)(由卷积层和全连接层交替组成，其中全连接层用1*1卷积层来代替)\n",
    "class NiN(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.net=nn.Sequential(\n",
    "            self.nin_block(1,96,11,4,0),\n",
    "            nn.MaxPool2d(3,2),\n",
    "            self.nin_block(96,256,5,1,2),\n",
    "            nn.MaxPool2d(3,2),\n",
    "            self.nin_block(256,384,3,1,1),\n",
    "            nn.MaxPool2d(3,2),\n",
    "            nn.Dropout(0.5),\n",
    "            self.nin_block(384,10,3,1,1),\n",
    "        )\n",
    "    def forward(self,img):\n",
    "        tmp=self.net(img)\n",
    "        output=F.avg_pool2d(tmp,tmp.shape[2:]).view(img.shape[0],-1)\n",
    "        return output\n",
    "    def nin_block(self,in_channels,out_channels,kernel_size,stride,padding):\n",
    "        blk=nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels,out_channels,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels,out_channels,1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        return blk\n",
    "\n",
    "#含并行连结的网络（GoogLeNet)\n",
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        class Inception(nn.Module):\n",
    "            def __init__(self, in_c,c1,c2,c3,c4):\n",
    "                super().__init__()\n",
    "                self.p1=nn.Sequential(\n",
    "                    nn.Conv2d(in_c,c1,1),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                self.p2=nn.Sequential(\n",
    "                    nn.Conv2d(in_c,c2[0],1),\n",
    "                    nn.Conv2d(c2[0],c2[1],3,padding=1),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                self.p3=nn.Sequential(\n",
    "                    nn.Conv2d(in_c,c3[0],1),\n",
    "                    nn.Conv2d(c3[0],c3[1],5,padding=2),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                self.p4=nn.Sequential(\n",
    "                    nn.MaxPool2d(3,stride=1,padding=1),\n",
    "                    nn.Conv2d(in_c,c4,1),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            def forward(self,x):\n",
    "                p1=self.p1(x)\n",
    "                p2=self.p2(x)\n",
    "                p3=self.p3(x)\n",
    "                p4=self.p4(x)\n",
    "                return torch.cat((p1,p2,p3,p4),dim=1)\n",
    "        class GlobalAvgPool2d(nn.Module):\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "            def forward(self,x):\n",
    "                return F.avg_pool2d(x,x.shape[2:]).view(x.shape[0],-1)\n",
    "        self.b1=nn.Sequential(\n",
    "            nn.Conv2d(1,64,7,2,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3,2,1)\n",
    "        )\n",
    "        self.b2=nn.Sequential(\n",
    "            nn.Conv2d(64,64,1),\n",
    "            nn.Conv2d(64,192,3,1),\n",
    "            nn.MaxPool2d(3,2,1)\n",
    "        )\n",
    "        self.b3=nn.Sequential(\n",
    "            Inception(192,64,(96,128),(16,32),32),\n",
    "            Inception(256,128,(128,192),(32,96),64),\n",
    "            nn.MaxPool2d(3,2,1)\n",
    "        )\n",
    "        self.b4=nn.Sequential(\n",
    "            Inception(480,192,(96,208),(16,48),64),\n",
    "            Inception(512,160,(112,224),(24,64),64),\n",
    "            Inception(512,128,(128,256),(24,64),64),\n",
    "            Inception(512,112,(144,288),(32,64),64),\n",
    "            Inception(528,256,(160,320),(32,128),128),\n",
    "            nn.MaxPool2d(3,2,1)\n",
    "        )\n",
    "        self.b5=nn.Sequential(\n",
    "            Inception(832,256,(160,320),(32,128),128),\n",
    "            Inception(832,384,(192,384),(48,128),128),\n",
    "            GlobalAvgPool2d()\n",
    "        )\n",
    "        self.net=nn.Sequential(\n",
    "            self.b1,\n",
    "            self.b2,\n",
    "            self.b3,\n",
    "            self.b4,\n",
    "            self.b5,\n",
    "            nn.Linear(1024,10)\n",
    "        )\n",
    "    def forward(self,img):\n",
    "        return self.net(img)\n",
    "\n",
    "#残差网络(ResNet-18)\n",
    "class ResNet(nn.Module):\n",
    "    class Residual(nn.Module):\n",
    "            def __init__(self, in_channels,out_channels,stride=1):\n",
    "                super().__init__()\n",
    "                self.residual=nn.Sequential(\n",
    "                    nn.Conv2d(in_channels,out_channels,3,stride,1),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv2d(out_channels,out_channels,3,padding=1),\n",
    "                    nn.BatchNorm2d(out_channels)\n",
    "                )\n",
    "                if in_channels!=out_channels:\n",
    "                    self.change=nn.Conv2d(in_channels,out_channels,1,stride)\n",
    "                else:\n",
    "                    self.change=None\n",
    "            def forward(self,x):\n",
    "                y=self.residual(x)\n",
    "                if self.change:\n",
    "                    x=self.change(x)\n",
    "                return F.relu(y+x)\n",
    "    class GlobalAvgPool2d(nn.Module):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "        def forward(self,x):\n",
    "            return F.avg_pool2d(x,x.shape[2:]).view(x.shape[0],-1)\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Conv2d(1,64,7,2,3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3,2,1),\n",
    "            self.resnet_block(64,64,2,True),\n",
    "            self.resnet_block(64,128,2),\n",
    "            self.resnet_block(128,256,2),\n",
    "            self.resnet_block(256,512,2),\n",
    "            self.GlobalAvgPool2d(),\n",
    "            nn.Linear(512,10)\n",
    "        )\n",
    "    def forward(self,img):\n",
    "        return self.net(img)\n",
    "    def resnet_block(self,in_channels,out_channels,num_residuals,first_block=False):\n",
    "        blk=[]\n",
    "        for i in range(num_residuals):\n",
    "            if i==0 and not first_block:\n",
    "                blk.append(self.Residual(in_channels,out_channels,stride=2))\n",
    "            else:\n",
    "                blk.append(self.Residual(out_channels,out_channels))\n",
    "        return nn.Sequential(*blk)\n",
    "\n",
    "#稠密连接网络(DenseNet)\n",
    "class DenseNet(nn.Module):\n",
    "    class DenseBlock(nn.Module):\n",
    "        def __init__(self,num_convs,in_channels,out_channels):\n",
    "            super().__init__()\n",
    "            net=[]\n",
    "            self.out_channels=in_channels\n",
    "            for i in range(num_convs):\n",
    "                net.append(self.conv_block(self.out_channels,out_channels))\n",
    "                self.out_channels+=out_channels\n",
    "            self.net=nn.ModuleList(net)\n",
    "        def forward(self,X):\n",
    "            for blk in self.net:\n",
    "                Y=blk(X)\n",
    "                X=torch.cat((X,Y),dim=1)\n",
    "            return X\n",
    "        def conv_block(self,in_channels,out_channels):\n",
    "            blk=nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels,out_channels,3,padding=1)\n",
    "            )\n",
    "            return blk\n",
    "    class GlobalAvgPool2d(nn.Module):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "        def forward(self,x):\n",
    "            return F.avg_pool2d(x,x.shape[2:]).view(x.shape[0],-1)\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Conv2d(1,64,7,2,3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3,2,1),\n",
    "        )\n",
    "        num_channels,growth_rate=64,32\n",
    "        num_convs_in_dense_blocks=[4,4,4,4]\n",
    "        for i,num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "            DB=self.DenseBlock(num_convs,num_channels,growth_rate)\n",
    "            self.net.add_module(f\"DenseBlock_{i}\",DB)\n",
    "            num_channels=DB.out_channels\n",
    "            if i!=len(num_convs_in_dense_blocks)-1:\n",
    "                self.net.add_module(f\"transition_block{i}\",self.transition_block(num_channels,num_channels//2))\n",
    "                num_channels=num_channels//2\n",
    "        self.net.add_module(\"BN\",nn.BatchNorm2d(num_channels))\n",
    "        self.net.add_module(\"relu\",nn.ReLU())\n",
    "        self.net.add_module(\"global_avg_pool\",self.GlobalAvgPool2d())\n",
    "        self.net.add_module(\"fc\",nn.Linear(num_channels,10))\n",
    "    def forward(self,img):\n",
    "        return self.net(img)\n",
    "    def transition_block(self,in_channels,out_channels):\n",
    "        blk=nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels,out_channels,1),\n",
    "            nn.AvgPool2d(2,2)\n",
    "        )\n",
    "        return blk\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评估模型\n",
    "def evaluate_accuracy(data_iter,net):\n",
    "    acc_sum,n=0.0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in data_iter:\n",
    "            acc_sum+=(net(X).argmax(dim=1)==y).float().sum().item()\n",
    "            n+=y.shape[0]\n",
    "    return acc_sum/n\n",
    "#训练模型\n",
    "def train_model(net,train_iter,loss,optimizer,num_epochs,device=None):\n",
    "    print(f\"training on {device}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        n,train_acc=0,0.0 \n",
    "        for X,y in train_iter:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            net=net.to(device)\n",
    "            y_hat=net(X)\n",
    "            l=loss(y_hat,y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_acc+=(y_hat.argmax(dim=1)==y).float().sum().item()\n",
    "            l*=y.shape[0]\n",
    "            n+=y.shape[0]\n",
    "        print(f\"epoch:{epoch},loss:{l/n},train_acc:{train_acc/n}\")\n",
    "    print(\"training end!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (DenseBlock_0): DenseBlock(\n",
      "      (net): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (transition_block0): Sequential(\n",
      "      (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (DenseBlock_1): DenseBlock(\n",
      "      (net): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (transition_block1): Sequential(\n",
      "      (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(224, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (DenseBlock_2): DenseBlock(\n",
      "      (net): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(112, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(144, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(176, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(208, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (transition_block2): Sequential(\n",
      "      (0): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(240, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (DenseBlock_3): DenseBlock(\n",
      "      (net): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(120, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): BatchNorm2d(152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(152, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(184, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(216, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (BN): BatchNorm2d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU()\n",
      "    (global_avg_pool): GlobalAvgPool2d()\n",
      "    (fc): Linear(in_features=248, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "training on cuda\n",
      "epoch:0,loss:0.0005819859798066318,train_acc:0.8391333333333333\n",
      "epoch:1,loss:0.0004254267842043191,train_acc:0.9009166666666667\n",
      "epoch:2,loss:0.00024094129912555218,train_acc:0.9147666666666666\n",
      "epoch:3,loss:0.0004936232580803335,train_acc:0.9227\n",
      "epoch:4,loss:0.0004499187634792179,train_acc:0.9305666666666667\n",
      "epoch:5,loss:0.00022886261285748333,train_acc:0.9352166666666667\n",
      "epoch:6,loss:0.0002638726436998695,train_acc:0.9383166666666667\n",
      "epoch:7,loss:0.00036188081139698625,train_acc:0.9421666666666667\n",
      "epoch:8,loss:0.00022254699433688074,train_acc:0.9482\n",
      "epoch:9,loss:8.186866762116551e-05,train_acc:0.9505666666666667\n",
      "epoch:10,loss:0.0002342815132578835,train_acc:0.9534166666666667\n",
      "epoch:11,loss:0.00029647769406437874,train_acc:0.9565666666666667\n",
      "epoch:12,loss:0.00012177130702184513,train_acc:0.9606333333333333\n",
      "epoch:13,loss:0.00015804768190719187,train_acc:0.9634333333333334\n",
      "epoch:14,loss:0.0004438330652192235,train_acc:0.9671333333333333\n",
      "epoch:15,loss:9.843801672104746e-05,train_acc:0.96865\n",
      "epoch:16,loss:9.450684592593461e-05,train_acc:0.9726666666666667\n",
      "epoch:17,loss:2.4488637791364454e-05,train_acc:0.97605\n",
      "epoch:18,loss:9.803529974306002e-05,train_acc:0.97875\n",
      "epoch:19,loss:0.00024192796263378114,train_acc:0.9799166666666667\n",
      "epoch:20,loss:3.3027630706783384e-05,train_acc:0.9822166666666666\n",
      "epoch:21,loss:0.00013023712381254882,train_acc:0.9830666666666666\n",
      "epoch:22,loss:6.819253030698746e-05,train_acc:0.9839333333333333\n",
      "epoch:23,loss:3.1140283681452274e-05,train_acc:0.98495\n",
      "epoch:24,loss:5.457374209072441e-05,train_acc:0.9888666666666667\n",
      "epoch:25,loss:6.876167026348412e-05,train_acc:0.98895\n",
      "epoch:26,loss:4.747826824313961e-05,train_acc:0.9886333333333334\n",
      "epoch:27,loss:9.862960723694414e-05,train_acc:0.99035\n",
      "epoch:28,loss:5.2908246289007366e-05,train_acc:0.9914666666666667\n",
      "epoch:29,loss:7.607806037412956e-05,train_acc:0.9914166666666666\n",
      "epoch:30,loss:0.00013756428961642087,train_acc:0.9923666666666666\n",
      "epoch:31,loss:1.0713124538597185e-05,train_acc:0.9927166666666667\n",
      "epoch:32,loss:7.327201456064358e-05,train_acc:0.9938\n",
      "epoch:33,loss:2.8436799766495824e-05,train_acc:0.993\n",
      "epoch:34,loss:2.727425999182742e-05,train_acc:0.99445\n",
      "epoch:35,loss:2.3103313651517965e-05,train_acc:0.9949833333333333\n",
      "epoch:36,loss:0.00020578823750838637,train_acc:0.9925166666666667\n",
      "epoch:37,loss:7.189474126789719e-05,train_acc:0.9937\n",
      "epoch:38,loss:2.664994462975301e-05,train_acc:0.9931833333333333\n",
      "epoch:39,loss:3.727501280081924e-06,train_acc:0.9949666666666667\n",
      "epoch:40,loss:4.236111635691486e-05,train_acc:0.99675\n",
      "epoch:41,loss:1.1032392649212852e-05,train_acc:0.99625\n",
      "epoch:42,loss:1.6927308024605736e-05,train_acc:0.9952666666666666\n",
      "epoch:43,loss:6.228336860658601e-05,train_acc:0.99385\n",
      "epoch:44,loss:2.5866182113531977e-05,train_acc:0.9965\n",
      "epoch:45,loss:1.1097270544269122e-05,train_acc:0.99815\n",
      "epoch:46,loss:1.3533256606024224e-05,train_acc:0.9933833333333333\n",
      "epoch:47,loss:1.4665291928395163e-05,train_acc:0.99485\n",
      "epoch:48,loss:5.557440090342425e-06,train_acc:0.99545\n",
      "epoch:49,loss:9.738552762428299e-06,train_acc:0.9983666666666666\n",
      "training end!\n",
      "0.9341\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "此区域用于测试模型\n",
    "'''\n",
    "#该段代码用于LeNet神经网络测试\n",
    "\"\"\"\n",
    "net=LeNet()\n",
    "batch_size=256\n",
    "#获取数据集和训练模型\n",
    "mnist_train=torchvision.datasets.FashionMNIST(root=\"./DataSets/FashionMNIST\",train=True,download=True,transform=transforms.ToTensor())\n",
    "mnist_test=torchvision.datasets.FashionMNIST(root=\"./DataSets/FashionMNIST\",train=False,download=True,transform=transforms.ToTensor())\n",
    "train_iter=torch.utils.data.DataLoader(mnist_train,batch_size,True)\n",
    "test_iter=torch.utils.data.DataLoader(mnist_test,batch_size,False)\n",
    "#定义设备，损失函数，优化器，训练次数\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "num_epochs=20\n",
    "train_model(net,train_iter,loss,optimizer,num_epochs,device=device)\n",
    "print(evaluate_accuracy(test_iter,net.cpu()))\n",
    "\"\"\"\n",
    "#该段代码用于AlexNet深度神经网络测试\n",
    "\"\"\"\n",
    "net=AlexNet()\n",
    "batch_size=256\n",
    "#获取数据集和训练模型\n",
    "#变换列表并组合\n",
    "trans=[]\n",
    "trans.append(torchvision.transforms.Resize(224))\n",
    "trans.append(torchvision.transforms.ToTensor())\n",
    "transform=torchvision.transforms.Compose(trans)\n",
    "mnist_train=torchvision.datasets.FashionMNIST(root=\"./DataSets/FashionMNIST\",train=True,download=True,transform=transform)\n",
    "mnist_test=torchvision.datasets.FashionMNIST(root=\"./DataSets/FashionMNIST\",train=False,download=True,transform=transform)\n",
    "train_iter=torch.utils.data.DataLoader(mnist_train,batch_size,True)\n",
    "test_iter=torch.utils.data.DataLoader(mnist_test,batch_size,False)\n",
    "#定义设备，损失函数，优化器，训练次数\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "num_epochs=50\n",
    "train_model(net,train_iter,loss,optimizer,num_epochs)\n",
    "evaluate_accuracy(test_iter,net.cpu())\n",
    "\"\"\"\n",
    "#该段代码用于VGG-11(使用重复元素的神经网络)测试\n",
    "\"\"\"\n",
    "net=VGG()\n",
    "print(net)\n",
    "batch_size=256\n",
    "#获取数据集和训练模型\n",
    "#变换列表并组合\n",
    "trans=[]\n",
    "trans.append(torchvision.transforms.Resize(224))\n",
    "trans.append(torchvision.transforms.ToTensor())\n",
    "transform=torchvision.transforms.Compose(trans)\n",
    "mnist_train=torchvision.datasets.FashionMNIST(root=\"./DataSets/FashionMNIST\",train=True,download=True,transform=transform)\n",
    "mnist_test=torchvision.datasets.FashionMNIST(root=\"./DataSets/FashionMNIST\",train=False,download=True,transform=transform)\n",
    "train_iter=torch.utils.data.DataLoader(mnist_train,batch_size,True)\n",
    "test_iter=torch.utils.data.DataLoader(mnist_test,batch_size,False)\n",
    "#定义设备，损失函数，优化器，训练次数\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "num_epochs=50\n",
    "train_model(net,train_iter,loss,optimizer,num_epochs,device=device)\n",
    "evaluate_accuracy(test_iter,net.cpu())\n",
    "\"\"\"\n",
    "#该段代码用于NiN测试\n",
    "\"\"\"\n",
    "net=NiN()\n",
    "print(net)\n",
    "batch_size=256\n",
    "#获取数据集和训练模型\n",
    "#变换列表并组合\n",
    "trans=[]\n",
    "trans.append(torchvision.transforms.Resize(224))\n",
    "trans.append(torchvision.transforms.ToTensor())\n",
    "transform=torchvision.transforms.Compose(trans)\n",
    "mnist_train=torchvision.datasets.FashionMNIST(root=\"./DataSets/FashionMNIST\",train=True,download=True,transform=transform)\n",
    "mnist_test=torchvision.datasets.FashionMNIST(root=\"./DataSets/FashionMNIST\",train=False,download=True,transform=transform)\n",
    "train_iter=torch.utils.data.DataLoader(mnist_train,batch_size,True)\n",
    "test_iter=torch.utils.data.DataLoader(mnist_test,batch_size,False)\n",
    "#定义设备，损失函数，优化器，训练次数\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "num_epochs=50\n",
    "train_model(net,train_iter,loss,optimizer,num_epochs,device=device)\n",
    "evaluate_accuracy(test_iter,net.cpu())\n",
    "\"\"\"\n",
    "#该段代码用于GoogLeNet测试\n",
    "\"\"\"\n",
    "net=GoogLeNet()\n",
    "print(net)\n",
    "batch_size=256\n",
    "#获取数据集和训练模型\n",
    "#变换列表并组合\n",
    "trans=[]\n",
    "trans.append(torchvision.transforms.Resize(96))\n",
    "trans.append(torchvision.transforms.ToTensor())\n",
    "transform=torchvision.transforms.Compose(trans)\n",
    "mnist_train=torchvision.datasets.FashionMNIST(root=\"./DataSets/FashionMNIST\",train=True,download=True,transform=transform)\n",
    "mnist_test=torchvision.datasets.FashionMNIST(root=\"./DataSets/FashionMNIST\",train=False,download=True,transform=transform)\n",
    "train_iter=torch.utils.data.DataLoader(mnist_train,batch_size,True)\n",
    "test_iter=torch.utils.data.DataLoader(mnist_test,batch_size,False)\n",
    "#定义设备，损失函数，优化器，训练次数\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "num_epochs=50\n",
    "train_model(net,train_iter,loss,optimizer,num_epochs,device=device)\n",
    "evaluate_accuracy(test_iter,net.cpu())\n",
    "\"\"\"\n",
    "#该段代码用于ResNet测试\n",
    "\"\"\"\n",
    "net=ResNet()\n",
    "print(net)\n",
    "batch_size=256\n",
    "#获取数据集和训练模型\n",
    "#变换列表并组合\n",
    "trans=[]\n",
    "trans.append(torchvision.transforms.Resize(96))\n",
    "trans.append(torchvision.transforms.ToTensor())\n",
    "transform=torchvision.transforms.Compose(trans)\n",
    "mnist_train=torchvision.datasets.FashionMNIST(root=\"./DataSets/FashionMNIST\",train=True,download=True,transform=transform)\n",
    "mnist_test=torchvision.datasets.FashionMNIST(root=\"./DataSets/FashionMNIST\",train=False,download=True,transform=transform)\n",
    "train_iter=torch.utils.data.DataLoader(mnist_train,batch_size,True)\n",
    "test_iter=torch.utils.data.DataLoader(mnist_test,batch_size,False)\n",
    "#定义设备，损失函数，优化器，训练次数\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "num_epochs=50\n",
    "train_model(net,train_iter,loss,optimizer,num_epochs,device=device)\n",
    "evaluate_accuracy(test_iter,net.cpu())\n",
    "\"\"\"\n",
    "#该段代码用于DenseNet测试\n",
    "net=DenseNet()\n",
    "print(net)\n",
    "batch_size=256\n",
    "#获取数据集和训练模型\n",
    "#变换列表并组合\n",
    "trans=[]\n",
    "trans.append(torchvision.transforms.Resize(96))\n",
    "trans.append(torchvision.transforms.ToTensor())\n",
    "transform=torchvision.transforms.Compose(trans)\n",
    "mnist_train=torchvision.datasets.FashionMNIST(root=\"./DataSets/FashionMNIST\",train=True,download=True,transform=transform)\n",
    "mnist_test=torchvision.datasets.FashionMNIST(root=\"./DataSets/FashionMNIST\",train=False,download=True,transform=transform)\n",
    "train_iter=torch.utils.data.DataLoader(mnist_train,batch_size,True)\n",
    "test_iter=torch.utils.data.DataLoader(mnist_test,batch_size,False)\n",
    "#定义设备，损失函数，优化器，训练次数\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=0.001)\n",
    "num_epochs=50\n",
    "train_model(net,train_iter,loss,optimizer,num_epochs,device=device)\n",
    "print(evaluate_accuracy(test_iter,net.cpu()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n此处用于做个人代码测试\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "此处用于做个人代码测试\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
